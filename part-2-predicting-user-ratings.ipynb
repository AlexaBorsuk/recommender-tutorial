{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Predicting user ratings\n",
    "\n",
    "In Part 1, we explored item-item recommendations which involve recommending new items that are similar to a given item. An example of an item-item recommendation is Netflix's \"Because you watched Movie X\", which recommends movies based on a recent movie (Movie X) that you watched.  \n",
    "\n",
    "In this tutorial, we will explore user-item recommendations. This approach is a more personalized type of recommendation which specifically predicts a user's degree of preference (e.g., rating) towards a given item. We will use a technique called **matrix factorization**. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv(\"data/ratings.csv\")\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "For a more comprehensive exploratory data analysis, please see `Exploratory Data Analysis` in Part 1's notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ratings: 100836\n",
      "Number of unique movieId's: 9724\n",
      "Number of unique users: 610\n",
      "Average number of ratings per user: 165.3\n",
      "Average number of ratings per movie: 10.37\n"
     ]
    }
   ],
   "source": [
    "n_ratings = len(ratings)\n",
    "n_movies = ratings['movieId'].nunique()\n",
    "n_users = ratings['userId'].nunique()\n",
    "\n",
    "print(f\"Number of ratings: {n_ratings}\")\n",
    "print(f\"Number of unique movieId's: {n_movies}\")\n",
    "print(f\"Number of unique users: {n_users}\")\n",
    "print(f\"Average number of ratings per user: {round(n_ratings/n_users, 2)}\")\n",
    "print(f\"Average number of ratings per movie: {round(n_ratings/n_movies, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a helper function called `create_X` which is able to transform the `ratings` dataframe into a n_users $\\times$ n_items sparse matrix. The function returns a user-item matrix of type `scipy.sparse.csr_matrix` (a sparse matrix), and four dictionaries that id's to indices (and vice versa):\n",
    "\n",
    "- **user_mapper:** user_id $\\longrightarrow$ user_index\n",
    "- **movie_mapper:** movie_id $\\longrightarrow$ movie_index \n",
    "- **user_inv_mapper:** user_index $\\longrightarrow$ user_id\n",
    "- **movie_inv_mapper:** movie_index $\\longrightarrow$ movie_id \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_X\n",
    "\n",
    "X, user_mapper, movie_mapper, user_inv_mapper, movie_inv_mapper = create_X(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9724, 610)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix sparsity: 1.70%\n"
     ]
    }
   ],
   "source": [
    "sparsity = X.count_nonzero()/(X.shape[0]*X.shape[1])\n",
    "\n",
    "print(\"Matrix sparsity: {0:.2f}%\".format(sparsity*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the user-item matrix is quite sparse. This means that there are few user-item ratings in our matrix relative to the total number of users and movies in our dataset, which makes sense given that a user typically rates only a handful of movies. If a user hasn't watched a particular movie, that interaction is an empty cell or a \"missing\" value. We are going to predict the ratings that would replace the missing values in this matrix using a technique called matrix factorization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is matrix factorization?\n",
    "\n",
    "Matrix factorization is a concept from **linear algebra** that decomposes a large matrix into a set of small matrices. In this case, we want to decompose our user-item matrix into two matrices with the condensing our matrix into more meaningful $k$ latent features:\n",
    "\n",
    "<img src=\"images/marix-factorization.png\"/>\n",
    "\n",
    "\n",
    "We are going to predict a user's rating of a movie using matrix factorization. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=200, n_iter=10, random_state=42)\n",
    "Z = svd.fit_transform(X)\n",
    "new_X = svd.inverse_transform(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9724, 610)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 862,  989,  899, 1223,   46])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X[:,0].argsort()[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.470953683291143"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X[862,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "How many latent features (n_components) do we select for our model? Though it isn't possible to calculate optimal `n_components` with an equation, we can attempt to identify the best `n_comopnents` empirically by testing out different values of `n_components` and see which `n_components` value scores best.\n",
    "\n",
    "#### 1. Identifying a robust evaluation metric\n",
    "\n",
    "#### 2. Splitting our data into training and validation sets\n",
    "\n",
    "#### 3. Performing cross-validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
